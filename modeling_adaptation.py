"""
Implementation of the bottleneck adaptation and FiLM adaptation.
A adaptation network is used to generate the parameters of adapters given a task
representation.
The implementation of adaptation network for FiLM is borrowed from:
    https://github.com/cambridge-mlg/cnaps
"""
from __future__ import (absolute_import, division, print_function,
                        unicode_literals)

import sys

from torch.nn.modules.activation import ELU
from utils import extract_indices, mean_pooling

import torch
from torch import nn
from torch.nn.modules.linear import Linear
import torch.nn.functional as F

from transformers import (ACT2FN)
from logging_utils import get_logger
logger = get_logger('CNAP')

###############################################
# Bottleneck adaptation                       #
# Reference: https://arxiv.org/abs/1902.00751 #
###############################################
class BottleneckAdapter(nn.Module):
    """ Bottleneck layer for model adaptation, with adaptation on the middle layer.
    """
    def __init__(self, config):
        super().__init__()
        self.down_proj_layer = nn.Linear(config.hidden_size,
                                        config.bn_adapter_hidden_size)
        self.up_proj_layer = nn.Linear(config.bn_adapter_hidden_size,
                                    config.hidden_size)
        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):
            self.adapter_act_fn = ACT2FN[config.hidden_act]
        else:
            self.adapter_act_fn = config.hidden_act
        self.layer_norm_n_shape = (config.hidden_size, ) 
        self.layer_norm_eps=config.layer_norm_eps
    
    def forward_given_parameters(self, input_tensor, params):
        hidden_states = F.linear(input_tensor, params['down_proj_weight'], params['down_proj_bias'])
        hidden_states = self.adapter_act_fn(hidden_states)
        output_states = F.linear(hidden_states, params['up_proj_weight'], params['up_proj_bias'])
        output_states = F.layer_norm(output_states, self.layer_norm_n_shape,
                                        params['layer_norm_weight'],
                                        params['layer_norm_bias'],
                                        eps=self.layer_norm_eps)
        return output_states+input_tensor, None

    def forward(self, input_tensor, params=None, return_hidden=False):
        if params is None or 'use_hyper_net' not in params or not params['use_hyper_net']:
            # forward pass using own parameters
            hidden_states = self.down_proj_layer(input_tensor)
            hidden_states = self.adapter_act_fn(hidden_states)
            if params is not None and 'scale' in params:
                hidden_states = hidden_states * params['scale']
            if params is not None and 'shift' in params:
                hidden_states = hidden_states + params['shift']
            output_states = self.up_proj_layer(hidden_states)

            return output_states+input_tensor, None
        else:
            # forward pass using parameters generated by hypernetwork
            return self.forward_given_parameters(input_tensor, params)

    def linear_layer_init_with_zeros(self, input_size, output_size):
        l = nn.Linear(input_size, output_size)
        l.weight.data.fill_(0.)
        l.bias.data.fill_(0.)
        return l

    def mlp_init_last_with_zeros(self, input_size, output_size):
        last_linear = nn.Linear(input_size // 2, output_size)
        last_linear.weight.data.fill_(0.)
        last_linear.bias.data.fill_(0.)
        m_list = nn.Sequential(
            nn.Linear(input_size, input_size // 2),
            nn.ELU(),
            last_linear)
        return m_list

class BottleneckAdapterAdaptInputOutput(BottleneckAdapter):
    """ Bottleneck layer for model adaptation.
    """
    def forward(self, input_tensor, params=None, return_hidden=False):
        if params is not None:
            input_tensor = input_tensor * params['scale_in'] + params['shift_in'][0]
        hidden_states = self.down_proj_layer(input_tensor)
        hidden_states = self.adapter_act_fn(hidden_states)
        output_states = self.up_proj_layer(hidden_states)
        if params is not None:
            output_states = output_states * params['scale_out'] + params['shift_out']
        if return_hidden:
            raise NotImplementedError()
            input_tensor.retain_grad()
            hidden_states.retain_grad()
            output_states.retain_grad()
            return output_states+input_tensor, (input_tensor, hidden_states, output_states)
        else:
            return output_states+input_tensor, None

class BottleneckAdapterAdaptInputOutputWithTaskEmb(BottleneckAdapter):
    """ Bottleneck layer for model adaptation.
    """
    def __init__(self, config):
        super().__init__(config)
        self.shift_in_mlp = nn.Sequential(
            nn.Linear(config.hidden_size + config.task_emb_size, 10),
            nn.ELU(),
            nn.Linear(10, 1)
        )
        self.shift_in_reg = torch.nn.Parameter(
            torch.nn.init.normal_(torch.empty(128), 0, 0.001), requires_grad=True)
        self.shift_out_mlp = nn.Sequential(
            nn.Linear(config.hidden_size + config.task_emb_size, 10),
            nn.ELU(),
            nn.Linear(10, 1)
        )
        self.shift_out_reg = torch.nn.Parameter(
            torch.nn.init.normal_(torch.empty(128), 0, 0.001), requires_grad=True)
        self.scale_in_mlp = nn.Sequential(
            nn.Linear(config.hidden_size + config.task_emb_size, 10),
            nn.ELU(),
            nn.Linear(10, 1)
        )
        self.scale_in_reg = torch.nn.Parameter(
            torch.nn.init.normal_(torch.empty(128), 0, 0.001), requires_grad=True)
        self.scale_out_mlp = nn.Sequential(
            nn.Linear(config.hidden_size + config.task_emb_size, 10),
            nn.ELU(),
            nn.Linear(10, 1)
        )
        self.scale_out_reg = torch.nn.Parameter(
            torch.nn.init.normal_(torch.empty(128), 0, 0.001), requires_grad=True)

    def regularization_term(self):
        l2_term = 0
        l2_term += (self.scale_in_reg ** 2).sum()
        l2_term += (self.shift_in_reg ** 2).sum()
        l2_term += (self.scale_out_reg ** 2).sum()
        l2_term += (self.shift_out_reg ** 2).sum()
        return l2_term

    def forward(self, input_tensor, params=None, return_hidden=False):
        if params is not None:
            task_emb = params['task_emb'].reshape(1,1,-1)
            bsize_slen = (input_tensor.shape[0], input_tensor.shape[1])
            task_emb = task_emb.expand(*bsize_slen, -1)

            scale_in = self.scale_in_mlp(torch.cat((input_tensor, task_emb), dim=-1).reshape(bsize_slen[0]*bsize_slen[1], -1)).reshape(*bsize_slen, -1)
            shift_in = self.shift_in_mlp(torch.cat((input_tensor, task_emb), dim=-1).reshape(bsize_slen[0]*bsize_slen[1], -1)).reshape(*bsize_slen, -1)
            scale_in = scale_in * self.scale_in_reg.unsqueeze(0).unsqueeze(-1) + torch.ones_like(scale_in)
            shift_in = shift_in * self.shift_in_reg.unsqueeze(0).unsqueeze(-1)
            input_tensor = input_tensor * scale_in + shift_in

        hidden_states = self.down_proj_layer(input_tensor)
        hidden_states = self.adapter_act_fn(hidden_states)
        output_states = self.up_proj_layer(hidden_states)
        if params is not None:
            scale_out = self.scale_out_mlp(torch.cat((output_states, task_emb), dim=-1).reshape(bsize_slen[0]*bsize_slen[1], -1)).reshape(*bsize_slen, -1)
            shift_out = self.shift_out_mlp(torch.cat((output_states, task_emb), dim=-1).reshape(bsize_slen[0]*bsize_slen[1], -1)).reshape(*bsize_slen, -1)
            scale_out = scale_out * self.scale_out_reg.unsqueeze(0).unsqueeze(-1) + torch.ones_like(scale_out)
            shift_out = shift_out * self.shift_out_reg.unsqueeze(0).unsqueeze(-1)
            output_states = output_states * scale_out + shift_out

        if return_hidden:
            raise NotImplementedError()
            input_tensor.retain_grad()
            hidden_states.retain_grad()
            output_states.retain_grad()
            return output_states+input_tensor, (input_tensor, hidden_states, output_states)
        else:
            return output_states+input_tensor, None

class BottleneckAdapterAdaptContextWithTaskEmb(BottleneckAdapter):
    """ Bottleneck layer for model adaptation.
    """
    def __init__(self, config):
        super().__init__(config)

        self.context_linear_mid = nn.Linear(config.task_emb_size, config.bn_adapter_hidden_size)
        self.context_linear_out = nn.Linear(config.task_emb_size, config.hidden_size)

    def forward(self, input_tensor, params=None, return_hidden=False):
        hidden_states = self.down_proj_layer(input_tensor)
        if params is not None:
            task_emb = params['task_emb'].reshape(1,-1)
            context_emb_mid = self.context_linear_mid(task_emb).unsqueeze(0)
            hidden_states = hidden_states + context_emb_mid
        hidden_states = self.adapter_act_fn(hidden_states)
        output_states = self.up_proj_layer(hidden_states)
        if params is not None:
            context_emb_out = self.context_linear_out(task_emb).unsqueeze(0)
            output_states = output_states + context_emb_out

        if return_hidden:
            raise NotImplementedError()
        else:
            return output_states+input_tensor, None

class BottleneckAdapterAdaptContextShiftScaleAR(BottleneckAdapter):
    """ Bottleneck layer for model adaptation.
    """
    def __init__(self, config):
        super().__init__(config)
        self.use_given_context_linears = hasattr(config, 'use_given_context_linears') and config.use_given_context_linears
        self.adapt_out = config.adapt_out
        if self.use_given_context_linears:
            logger.warning("context linears should be provided in forward pass for BottleneckAdapterAdaptContextWithTaskEmbCLSShiftScaleAR")
            pass
        else:
            _linear_method = self.linear_layer_init_with_zeros if not config.multilayer_hyper else self.mlp_init_last_with_zeros

            self.context_linear_mid_shift = _linear_method(
                                            config.task_emb_size + config.hidden_size,
                                            config.bn_adapter_hidden_size)
            self.context_linear_mid_scale = _linear_method(
                                            config.task_emb_size + config.hidden_size,
                                            config.bn_adapter_hidden_size)

            if self.adapt_out:
                self.context_linear_out_shift = _linear_method(
                                                config.task_emb_size + config.hidden_size,
                                                config.hidden_size)
                self.context_linear_out_scale = _linear_method(
                                                    config.task_emb_size + config.hidden_size,
                                                    config.hidden_size)

    def forward(self, input_tensor, params=None, return_hidden=False):
        if params is not None:
            if self.use_given_context_linears:
                context_linear_mid_shift = params['context_linear_mid_shift']
                context_linear_mid_scale = params['context_linear_mid_scale']
                context_linear_out_shift = params['context_linear_out_shift']
                context_linear_out_scale = params['context_linear_out_scale']
            else:
                context_linear_mid_shift = self.context_linear_mid_shift
                context_linear_mid_scale = self.context_linear_mid_scale
                context_linear_out_shift = self.context_linear_out_shift
                context_linear_out_scale = self.context_linear_out_scale

        b_size = input_tensor.shape[0]
        s_len = input_tensor.shape[1]
        hidden_states = self.down_proj_layer(input_tensor)
        if params is not None:
            task_emb = params['task_emb'].reshape(1, 1,-1).expand(b_size, s_len, -1) # b_size * seq_len * task_emb_size
            task_emb = torch.cat((task_emb, input_tensor), dim=-1) # b_size * seq_len * (task_emb_size+hidden_size)
            task_emb = task_emb.reshape(b_size*s_len, -1)

            context_emb_mid_shift = context_linear_mid_shift(task_emb).reshape(b_size, s_len, -1) # b_size * s_seq * h_size
            context_emb_mid_scale = context_linear_mid_scale(task_emb).reshape(b_size, s_len, -1) # b_size * s_seq * h_size

            context_emb_mid_scale = torch.tanh(context_emb_mid_scale) + 1

            hidden_states = hidden_states * context_emb_mid_scale + context_emb_mid_shift

        hidden_states = self.adapter_act_fn(hidden_states)
        output_states = self.up_proj_layer(hidden_states)
        if params is not None and self.adapt_out:
            context_emb_out_shift = context_linear_out_shift(task_emb).reshape(b_size, s_len, -1)
            context_emb_out_scale = context_linear_out_scale(task_emb).reshape(b_size, s_len, -1)
            context_emb_out_scale = torch.tanh(context_emb_out_scale) + 1

            output_states = output_states * context_emb_out_scale + context_emb_out_shift

        if return_hidden:
            raise NotImplementedError()
        else:
            return output_states+input_tensor, None

class BottleneckAdapterAdaptContextWithTaskEmbCLS(BottleneckAdapter):
    """ Bottleneck layer for model adaptation.
    """
    def __init__(self, config):
        super().__init__(config)

        self.context_linear_mid = nn.Linear(config.task_emb_size, config.bn_adapter_hidden_size)
        self.context_linear_out = nn.Linear(config.task_emb_size, config.hidden_size)

    def forward(self, input_tensor, params=None, return_hidden=False):
        hidden_states = self.down_proj_layer(input_tensor)
        if params is not None:
            task_emb = params['task_emb'].reshape(1,-1)
            context_emb_mid = self.context_linear_mid(task_emb)
            hidden_states[0] = hidden_states[0] + context_emb_mid
        hidden_states = self.adapter_act_fn(hidden_states)
        output_states = self.up_proj_layer(hidden_states)
        if params is not None:
            context_emb_out = self.context_linear_out(task_emb)
            output_states[0] = output_states[0] + context_emb_out

        if return_hidden:
            raise NotImplementedError()
        else:
            return output_states+input_tensor, None

class BottleneckAdapterAdaptContextWithTaskEmbCLSShiftScale(BottleneckAdapter):
    """ Bottleneck layer for model adaptation.
    """
    def __init__(self, config):
        super().__init__(config)
        self.use_given_context_linears = hasattr(config, 'use_given_context_linears') and config.use_given_context_linears
        if self.use_given_context_linears:
            # context linears will be provided in forward pass
            pass
        else:
            self.context_linear_mid_shift = nn.Linear(config.task_emb_size, config.bn_adapter_hidden_size)
            self.context_linear_mid_scale = nn.Linear(config.task_emb_size, config.bn_adapter_hidden_size)
            self.context_linear_out_shift = nn.Linear(config.task_emb_size, config.hidden_size)
            self.context_linear_out_scale = nn.Linear(config.task_emb_size, config.hidden_size)

    def forward(self, input_tensor, params=None, return_hidden=False):
        if self.use_given_context_linears:
            context_linear_mid_shift = params['context_linear_mid_shift']
            context_linear_mid_scale = params['context_linear_mid_scale']
            context_linear_out_shift = params['context_linear_out_shift']
            context_linear_out_scale = params['context_linear_out_scale']
        else:
            context_linear_mid_shift = self.context_linear_mid_shift
            context_linear_mid_scale = self.context_linear_mid_scale
            context_linear_out_shift = self.context_linear_out_shift
            context_linear_out_scale = self.context_linear_out_scale

        b_size = input_tensor.shape[0]
        s_len = input_tensor.shape[1]
        hidden_states = self.down_proj_layer(input_tensor)
        if params is not None:
            task_emb = params['task_emb'].reshape(1,-1)
            context_emb_mid_shift = context_linear_mid_shift(task_emb).unsqueeze(0) # 1 * 1 * h_size
            dummpy_zeros = torch.zeros_like(context_emb_mid_shift).expand(-1, s_len-1, -1)
            context_emb_mid_shift = torch.cat((context_emb_mid_shift, dummpy_zeros), dim=1)

            context_emb_mid_scale = context_linear_mid_scale(task_emb).unsqueeze(0) # 1 * 1 * h_size
            context_emb_mid_scale = torch.tanh(context_emb_mid_scale) + 1
            dummy_ones = torch.ones_like(context_emb_mid_scale).expand(-1, s_len-1, -1)
            context_emb_mid_scale = torch.cat((context_emb_mid_scale, dummy_ones), dim=1)

            hidden_states = hidden_states * context_emb_mid_scale + context_emb_mid_shift

        hidden_states = self.adapter_act_fn(hidden_states)
        output_states = self.up_proj_layer(hidden_states)
        if params is not None:
            context_emb_out_shift = context_linear_out_shift(task_emb).unsqueeze(0)
            dummpy_zeros = torch.zeros_like(context_emb_out_shift).expand(-1, s_len-1, -1)
            context_emb_out_shift = torch.cat((context_emb_out_shift, dummpy_zeros), dim=1)

            context_emb_out_scale = context_linear_out_scale(task_emb).unsqueeze(0)
            context_emb_out_scale = torch.tanh(context_emb_out_scale) + 1
            dummy_ones = torch.ones_like(context_emb_out_scale).expand(-1, s_len-1, -1)
            context_emb_out_scale = torch.cat((context_emb_out_scale, dummy_ones), dim=1)

            output_states = output_states * context_emb_out_scale + context_emb_out_shift

        if return_hidden:
            raise NotImplementedError()
        else:
            return output_states+input_tensor, None

class BottleneckAdapterAdaptContextWithTaskEmbCLSShiftScaleAR(BottleneckAdapter):
    """ Bottleneck layer for model adaptation.
    """
    def __init__(self, config):
        super().__init__(config)
        self.use_given_context_linears = hasattr(config, 'use_given_context_linears') and config.use_given_context_linears
        self.adapt_out = config.adapt_out
        if self.use_given_context_linears:
            logger.warning("context linears should be provided in forward pass for BottleneckAdapterAdaptContextWithTaskEmbCLSShiftScaleAR")
            pass
        else:
            _linear_method = self.linear_layer_init_with_zeros if not config.multilayer_hyper else self.mlp_init_last_with_zeros

            self.context_linear_mid_shift = _linear_method(
                                            config.task_emb_size + config.hidden_size,
                                            config.bn_adapter_hidden_size)
            self.context_linear_mid_scale = _linear_method(
                                            config.task_emb_size + config.hidden_size,
                                            config.bn_adapter_hidden_size)

            if self.adapt_out:
                self.context_linear_out_shift = _linear_method(
                                                config.task_emb_size + config.hidden_size,
                                                config.hidden_size)
                self.context_linear_out_scale = _linear_method(
                                                    config.task_emb_size + config.hidden_size,
                                                    config.hidden_size)

    def forward(self, input_tensor, params=None, return_hidden=False):
        if params is not None:
            if self.use_given_context_linears:
                context_linear_mid_shift = params['context_linear_mid_shift']
                context_linear_mid_scale = params['context_linear_mid_scale']
                context_linear_out_shift = params['context_linear_out_shift']
                context_linear_out_scale = params['context_linear_out_scale']
            else:
                context_linear_mid_shift = self.context_linear_mid_shift
                context_linear_mid_scale = self.context_linear_mid_scale
                context_linear_out_shift = self.context_linear_out_shift
                context_linear_out_scale = self.context_linear_out_scale

        b_size = input_tensor.shape[0]
        s_len = input_tensor.shape[1]
        hidden_states = self.down_proj_layer(input_tensor)
        if params is not None:
            task_emb = params['task_emb'].reshape(1,-1).expand(b_size, -1) # b_size * task_emb_size
            task_emb = torch.cat((task_emb, input_tensor[:, 0, :]), dim=-1) # b_size * (task_emb_size+hidden_size)

            context_emb_mid_shift = context_linear_mid_shift(task_emb).unsqueeze(1) # b_size * 1 * h_size
            dummpy_zeros = torch.zeros_like(context_emb_mid_shift).expand(-1, s_len-1, -1)
            context_emb_mid_shift = torch.cat((context_emb_mid_shift, dummpy_zeros), dim=1)

            context_emb_mid_scale = context_linear_mid_scale(task_emb).unsqueeze(1) # b_size * 1 * h_size
            context_emb_mid_scale = torch.tanh(context_emb_mid_scale) + 1
            dummy_ones = torch.ones_like(context_emb_mid_scale).expand(-1, s_len-1, -1)
            context_emb_mid_scale = torch.cat((context_emb_mid_scale, dummy_ones), dim=1)

            hidden_states = hidden_states * context_emb_mid_scale + context_emb_mid_shift

        hidden_states = self.adapter_act_fn(hidden_states)
        output_states = self.up_proj_layer(hidden_states)
        if params is not None and self.adapt_out:
            context_emb_out_shift = context_linear_out_shift(task_emb).unsqueeze(1)
            dummpy_zeros = torch.zeros_like(context_emb_out_shift).expand(-1, s_len-1, -1)
            context_emb_out_shift = torch.cat((context_emb_out_shift, dummpy_zeros), dim=1)

            context_emb_out_scale = context_linear_out_scale(task_emb).unsqueeze(1)
            context_emb_out_scale = torch.tanh(context_emb_out_scale) + 1
            dummy_ones = torch.ones_like(context_emb_out_scale).expand(-1, s_len-1, -1)
            context_emb_out_scale = torch.cat((context_emb_out_scale, dummy_ones), dim=1)

            output_states = output_states * context_emb_out_scale + context_emb_out_shift

        if return_hidden:
            return output_states+input_tensor, [{
                'mid_scale': context_emb_mid_scale,
                'mid_shift': context_emb_mid_shift,
                'out_scale': context_emb_out_scale,
                'out_shift': context_emb_out_shift
            }]
        else:
            return output_states+input_tensor, None

class BottleneckAdapterWithoutTaskAdaptationWithShiftSacle(BottleneckAdapter):
    def __init__(self, config):
        super().__init__(config)
        self.film_mid_shift = nn.Parameter(torch.zeros(config.bn_adapter_hidden_size,
                                                       dtype=torch.float32), requires_grad=True)
        self.film_mid_scale = nn.Parameter(torch.ones(config.bn_adapter_hidden_size,
                                                       dtype=torch.float32), requires_grad=True)
        self.film_out_shift = nn.Parameter(torch.zeros(config.hidden_size,
                                                       dtype=torch.float32), requires_grad=True)
        self.film_out_scale = nn.Parameter(torch.ones(config.hidden_size,
                                                       dtype=torch.float32), requires_grad=True)

    def forward(self, input_tensor, params=None, return_hidden=False):
        # forward pass using own parameters
        hidden_states = self.down_proj_layer(input_tensor)
        hidden_states = hidden_states * self.film_mid_scale.reshape(1, 1, -1)
        hidden_states = hidden_states + self.film_mid_shift.reshape(1, 1, -1)
        hidden_states = self.adapter_act_fn(hidden_states)

        hidden_states = self.up_proj_layer(hidden_states)
        hidden_states = hidden_states * self.film_out_scale.reshape(1, 1, -1)
        hidden_states = hidden_states + self.film_out_shift.reshape(1, 1, -1)

        return hidden_states+input_tensor, None
    
###############################################
# FiLM adaptation                             #
# Reference: https://arxiv.org/abs/1709.07871 #
###############################################
class DenseResidualLayer(nn.Module):
    """
    PyTorch like layer for standard linear layer with identity residual connection.
    :param num_features: (int) Number of input / output units for the layer.
    """
    def __init__(self, num_features):
        super(DenseResidualLayer, self).__init__()
        self.linear = nn.Linear(num_features, num_features)

    def forward(self, x):
        """
        Forward-pass through the layer. Implements the following computation:

                f(x) = f_theta(x) + x
                f_theta(x) = W^T x + b

        :param x: (torch.tensor) Input representation to apply layer to ( dim(x) = (batch, num_features) ).
        :return: (torch.tensor) Return f(x) ( dim(f(x) = (batch, num_features) ).
        """
        identity = x
        out = self.linear(x)
        out += identity
        return out

class FilmLayerAdaptationNetwork(nn.Module):
    """ Single adaptation network for generating the parameters of each transformer
        layer in the base network. Will be wrapped around by FilmAdaptationNetwork.
    """
    def __init__(self, task_rep_dim, num_attention_heaed):
        """
        Args:
            task_rep_dim (int): Dimension of the task representation.
            num_attention_heaed (int): [description]
        """
        super().__init__()
        self.task_rep_dim = task_rep_dim
        self.num_attention_head = num_attention_heaed

        # Initialize the generators (adaptation networks) and regularization
        # lists for each of the output params
        # We adapt three components in each transformer layer:
        #   1. representation of k, q, v, one set of gamma / beta for each head
        #   2. output of the linear transformation after self-attention
        #   3. output of the last linear transformation
        for name in ['k', 'q', 'v'] + ['self_linear', 'out_linear']:
            num_param = 1 if 'linear' in name else num_attention_heaed
            setattr(self, f'gamma_{name}_gen', self._make_gen_network(num_param))
            setattr(self, f'gamma_{name}_reg', self._norm_param(num_param))
            setattr(self, f'beta_{name}_gen', self._make_gen_network(num_param))
            setattr(self, f'beta_{name}_reg', self._norm_param(num_param))

    def _norm_param(self, dim):
        return torch.nn.Parameter(
                        torch.nn.init.normal_(torch.empty(dim), 0, 0.001),
                        requires_grad=True)

    def _make_gen_network(self, output_dim, activation=nn.ReLU):
        # Task representation is the input, set the dimension accordingly
        input_dim = self.task_rep_dim
        # The desired output dimension is very low, e.g. 1 or 12, so the hidden
        # dimension is set to be 10x of input dimension
        hidden_dim = 10 * output_dim
        return nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            activation(),
            DenseResidualLayer(hidden_dim),
            activation(),
            DenseResidualLayer(hidden_dim),
            activation(),
            DenseResidualLayer(hidden_dim),
            activation(),
            nn.Linear(hidden_dim, output_dim)
        )

    def _gen_params(self, x, key, plus_ones=True):
        gen_key = key + '_gen'
        reg_key = key + '_reg'
        params = getattr(self, gen_key)(x).squeeze() * getattr(self, reg_key)
        if plus_ones:
            params += torch.ones_like(getattr(self, reg_key))
        return params

    def forward(self, x):
        """
        Forward pass through adaptation network.
        :param x: (torch.tensor) Input representation to network (task level representation z).
        :return: (list::dictionaries) Dictionary for every block in layer. Dictionary contains all the parameters
                 necessary to adapt layer in base network. Base network is aware of dict structure and can pull params
                 out during forward pass.
        """
        block_param_dict = {}

        for name in ['k', 'q', 'v'] + ['self_linear', 'out_linear']:
            key = f'gamma_{name}'
            block_param_dict[key] = self._gen_params(x, key)
            key = f'beta_{name}'
            block_param_dict[key] = self._gen_params(x, key, False)

        return block_param_dict

    def regularization_term(self):
        """
        Compute the regularization term for the parameters. Recall, FiLM applies gamma * x + beta. As such, params
        gamma and beta are regularized to unity, i.e. ||gamma - 1||_2 and ||beta||_2.
        :return: (torch.tensor) Scalar for l2 norm for all parameters according to regularization scheme.
        """
        l2_term = 0
        for name in ['k', 'q', 'v'] + ['self_linear', 'out_linear']:
            gamma_reg = getattr(self, f'gamma_{name}_reg')
            beta_reg = getattr(self, f'beta_{name}_reg')
            l2_term += (gamma_reg ** 2).sum()
            l2_term += (beta_reg ** 2).sum()
        return l2_term

class FilmAdaptationNetwork(nn.Module):
    """ FiLM adaptation network (outputs FiLM adaptation parameters for all
        layers in a base feature extractor).
    """
    def __init__(self, task_rep_dim, num_target_layer, num_attention_head):
        """
        Args:
            task_rep_dim (int): Dimension of the task representaion (input to
                the adaptation network).
            num_target_layer (int): Number of target layers (Transforme layer in
                this project) to adapt.
            num_attention_head (int): Number of attention heads in each
                Transformer layer.
        """
        super().__init__()
        self.layers = self.get_layers(task_rep_dim, num_target_layer,
                                      num_attention_head)

    def get_layers(self, task_rep_dim, num_target_layer,
                   num_attention_heaed):
        """
        Loop over layers of base network and initialize adaptation network.
        :return: (nn.ModuleList) ModuleList containing the adaptation network for each layer in base network.
        """
        layers = nn.ModuleList()
        for _ in range(num_target_layer):
            layers.append(FilmLayerAdaptationNetwork(task_rep_dim,
                                                     num_attention_heaed))
        return layers

    def forward(self, x):
        """
        Forward pass through adaptation network to create list of adaptation parameters.
        :param x: (torch.tensor) (z -- task level representation for generating adaptation).
        :return: (list::adaptation_params_dict) Returns a list of adaptation dictionaries, one for each layer in base net.
        """
        # return [self.layers[layer](x) for layer in range(self.num_target_layers)]
        return [layer(x) for layer in self.layers]

    def regularization_term(self):
        """
        Simple function to aggregate the regularization terms from each of the layers in the adaptation network.
        :return: (torch.scalar) A order-0 torch tensor with the regularization term for the adaptation net params.
        """
        l2_term = 0
        for layer in self.layers:
            l2_term += layer.regularization_term()
        return l2_term

class DenseResidualBlock(nn.Module):
    """
    Wrapping a number of residual layers for residual block. Will be used as building block in FiLM hyper-networks.
    :param in_size: (int) Number of features for input representation.
    :param out_size: (int) Number of features for output representation.
    """
    def __init__(self, in_size, out_size):
        super(DenseResidualBlock, self).__init__()
        hidden_size = in_size // 2
        self.linear1 = nn.Linear(in_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, hidden_size)
        self.linear3 = nn.Linear(hidden_size, out_size)
        self.activation = nn.ELU()

    def forward(self, x):
        """
        Forward pass through residual block. Implements following computation:

                h = f3( f2( f1(x) ) ) + x
                or
                h = f3( f2( f1(x) ) )

                where fi(x) = Elu( Wi^T x + bi )

        :param x: (torch.tensor) Input representation to apply layer to ( dim(x) = (batch, in_size) ).
        :return: (torch.tensor) Return f(x) ( dim(f(x) = (batch, out_size) ).
        """
        identity = x
        out = self.linear1(x)
        out = self.activation(out)
        out = self.linear2(out)
        out = self.activation(out)
        out = self.linear3(out)
        if x.shape[-1] == out.shape[-1]:
            out += identity
        return out

class LinearClassifierAdaptationNetwork(nn.Module):
    """
    Versa-style adaptation network for linear classifier (see https://arxiv.org/abs/1805.09921 for full details).
    Representations of a class are first averaged before being mapped to classification parameters.
    :param d_theta: (int) Input / output feature dimensionality for layer.
    """
    def __init__(self, d_theta):
        super(LinearClassifierAdaptationNetwork, self).__init__()
        self.weight_means_processor = self._make_mean_dense_block(d_theta, d_theta)
        self.bias_means_processor = self._make_mean_dense_block(d_theta, 1)

    @staticmethod
    def _make_mean_dense_block(in_size, out_size):
        """
        Simple method for generating different types of blocks. Final code only uses dense residual blocks.
        :param in_size: (int) Input representation dimensionality.
        :param out_size: (int) Output representation dimensionality.
        :return: (nn.Module) Adaptation network parameters for outputting classification parameters.
        """
        return DenseResidualBlock(in_size, out_size)

    def forward(self, representation_dict):
        """ Forward pass through adaptation network. Returns classification parameters for task.
            class in the task.
        Args:
            representation_dict (dict<torch.tensors>): Dictionary containing class-level representations for each

        Returns:
            dict<torch.tensors>: Dictionary containing the weights and biases for the classification of each class
                 in the task. Model can extract parameters and build the classifier accordingly. Supports sampling if
                 ML-PIP objective is desired.
        """
        classifier_param_dict = {}
        class_weight_means = []
        class_bias_means = []

        # Extract and sort the label set for the task
        label_set = list(representation_dict.keys())
        label_set.sort()
        num_classes = len(label_set)

        # For each class, extract the representation and pass it through adaptation network to generate classification
        # params for that class. Store parameters in a list,
        for class_num in label_set:
            nu = representation_dict[class_num]
            class_weight_means.append(self.weight_means_processor(nu))
            class_bias_means.append(self.bias_means_processor(nu))

        # Save the parameters as torch tensors (matrix and vector) and add to dictionary
        classifier_param_dict['weight_mean'] = torch.cat(class_weight_means, dim=0)
        classifier_param_dict['bias_mean'] = torch.reshape(torch.cat(class_bias_means, dim=1), [num_classes, ])

        return classifier_param_dict

class LeopardLinearClassifierAdaptationNetwork(nn.Module):
    """
    Versa-style adaptation network for linear classifier.
    Representations of a class are first mapped to classification parameters and
    then are averaged.
    Used by Leopard.
    """
    def __init__(self, input_size, output_size):
        super().__init__()
        self.processor = self._make_dense_block(input_size, output_size+1)

    @staticmethod
    def _make_dense_block(in_size, out_size):
        """
        Simple method for generating different types of blocks. Final code only uses dense residual blocks.
        :param in_size: (int) Input representation dimensionality.
        :param out_size: (int) Output representation dimensionality.
        :return: (nn.Module) Adaptation network parameters for outputting classification parameters.
        """
        return nn.Sequential(
            nn.Linear(in_size, out_size),
            nn.Tanh(),
            nn.Linear(out_size, out_size)
        )

    def _extract_class_indices(self, labels, which_class):
        """
        Helper method to extract the indices of elements which have the specified label.
        :param labels: (torch.tensor) Labels of the context set.
        :param which_class: Label for which indices are extracted.
        :return: (torch.tensor) Indices in the form of a mask that indicate the locations of the specified label.
        """
        class_mask = torch.eq(labels, which_class)  # binary mask of labels equal to which_class
        class_mask_indices = torch.nonzero(class_mask, as_tuple=False)  # indices of labels equal to which class
        return torch.reshape(class_mask_indices, (-1,))  # reshape to be a 1D vector

    def forward(self, features, labels):
        """ Forward pass through adaptation network. Returns classification parameters for task.
            class in the task.
        Args:
            representation_dict (dict<torch.tensors>): Dictionary containing class-level representations for each

        Returns:
            dict<torch.tensors>: Dictionary containing the weights and biases for the classification of each class
                 in the task. Model can extract parameters and build the classifier accordingly. Supports sampling if
                 ML-PIP objective is desired.
        """
        features = self.processor(features)
        class_params = []

        for c in torch.unique(labels, sorted=True):
            # filter out feature vectors which have class c
            class_features = torch.index_select(features,
                                                0,
                                                self._extract_class_indices(labels, c))
            class_params.append(mean_pooling(class_features))

        class_params = torch.cat(class_params, dim=0)

        classifier_param_dict = {}
        # Save the parameters as torch tensors (matrix and vector) and add to dictionary
        classifier_param_dict['weight_mean'] = class_params[:, :-1]
        classifier_param_dict['bias_mean'] = class_params[:, -1]

        return classifier_param_dict

class PrototypeBuildingNetwork(nn.Module):
    """
    A simple classifer adaptation network used by ProtoNet.
    The weights for classifier are initialized as the prototypes of all classes.
    """
    def __init__(self):
        super().__init__()
        pass

    def forward(self, features, labels):
        """ Forward pass through adaptation network. Returns classification parameters for task.
            class in the task.
        Args:
            representation_dict (dict<torch.tensors>): Dictionary containing class-level representations for each

        Returns:
            dict<torch.tensors>: Dictionary containing the weights and biases for the classification of each class
                 in the task. Model can extract parameters and build the classifier accordingly. Supports sampling if
                 ML-PIP objective is desired.
        """
        class_params = []

        for c in torch.unique(labels, sorted=True):
            # filter out feature vectors which have class c
            class_features = torch.index_select(features,
                                                0,
                                                extract_indices(labels, c))
            class_params.append(mean_pooling(class_features))

        prototypes = torch.cat(class_params, dim=0)

        return prototypes
